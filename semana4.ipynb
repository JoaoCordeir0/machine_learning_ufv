{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SgeoE1zXxJG"
      },
      "source": [
        "#### Semana 4 - Aprendizado por reforço"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L0-G5B_Z7Si"
      },
      "source": [
        "### Apresentação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YKRElmEZ_jX"
      },
      "source": [
        "Vamos iniciar com um ambiente simples, um \"labirinto\" unidimensional.\n",
        "\n",
        "Imagine o labirinto abaixo:\n",
        "\n",
        "[$\\mathrm{Buraco}$] [$\\qquad$] [$\\qquad$] [$\\qquad$] [$\\qquad$]  [ $\\mathrm{Partida}$ ] [$\\qquad$] [$\\qquad$] [$\\qquad$] [ $\\mathrm{Chegada}$ ]\n",
        "\n",
        "O agente, que inicia sua trajetória na casa \"Partida\", por escolher entre duas ações:\n",
        "- $-1$: andar para trás;\n",
        "- $+1$: andar para frente.\n",
        "\n",
        "Cada vez que ele realiza uma ação, o agente recebe uma recompensa de -1 ponto. Quando ele atinge a casa de \"Chegada\", o agente recebe uma recompensa de 10 ponto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLEM1yB5Z-jB",
        "outputId": "fff78628-2bd1-4a71-8cb3-834ae25df666"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definição do ambiente\n",
        "goal_position = 10\n",
        "state = 0  # Posição inicial\n",
        "actions = [-1, 1]  # Andar para trás ou para frente\n",
        "episodes = 10 # Número de ações a serem realizadas\n",
        "\n",
        "# Interação do agente com o ambiente\n",
        "for step in range(episodes):\n",
        "    action = np.random.choice(actions)  # Escolha aleatória\n",
        "    state += action\n",
        "    reward = 10 if state == goal_position else -1\n",
        "    print(f\"Passo {step + 1}: Ação: {action}, Estado: {state}, Recompensa: {reward}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4K2ZP_6wGz1"
      },
      "source": [
        "### Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhT8FQqHvK44"
      },
      "source": [
        "Vamos otimizar o funcionamento do agente por meio do algoritmo Q-learning. Vamos utilizar o mesmo problema do labirinto unidimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW1Uv6JHnnDm",
        "outputId": "2e1ef8de-13bb-4256-da86-cc8424871ab3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parâmetros\n",
        "states = 4\n",
        "goal_position = states-1\n",
        "actions = 2  # 0: esquerda, 1: direita\n",
        "q_table = np.zeros((states, actions))\n",
        "alpha = 0.1  # Taxa de aprendizado\n",
        "gamma = 0.9  # Fator de desconto\n",
        "epsilon = 0.2  # Taxa de exploração\n",
        "\n",
        "# Ambiente simples\n",
        "rewards = -1*np.ones(states-1)  # Recompensa ao alcançar o estado final\n",
        "rewards = np.append(rewards, 10)\n",
        "\n",
        "print(f\"Os valores de recompensa por estado são:\\n{rewards}\\n\")\n",
        "print(f\"A tabela Q foi inicializada como:\\n{q_table}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNQuVUzGvbpQ"
      },
      "source": [
        "Com o vetor de recompensas do ambiente e a tabela Q do agente inicializados, vamos iteragir para termos as melhores escolhas do ambiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnCVf5mVb4OV"
      },
      "source": [
        "Imagine o labirinto abaixo:\n",
        "\n",
        "[ $\\mathrm{Partida}$ ] [$\\qquad$] [$\\qquad$] [$\\qquad$] [ $\\mathrm{Chegada}$ ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0Mc6dqowRy7",
        "outputId": "f6230421-662c-4787-ef6b-02fa19a34200"
      },
      "outputs": [],
      "source": [
        "# Treinamento\n",
        "for episode in range(1):\n",
        "    state = 0  # Estado inicial\n",
        "    print(f\"A tabela Q antes de iniciar o episódio:\\n{q_table}\")\n",
        "    while state != goal_position:\n",
        "        # Escolha da ação\n",
        "        if np.random.random() < epsilon:\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        # Próximo estado e recompensa\n",
        "        next_state = state + 1 if action == 1 else max(0, state - 1)\n",
        "        reward = rewards[next_state]\n",
        "\n",
        "        # Atualização Q-Learning\n",
        "        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "        state = next_state\n",
        "        print(f\"\\nA tabela Q foi atualizada como:\\n{q_table}\")\n",
        "\n",
        "# Resultados\n",
        "print(\"Tabela Q final:\")\n",
        "print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5VBgi12vtpv"
      },
      "source": [
        "Ao iteragir durante apenas 1 episódio, vemos que a tabela Q convergiu de acordo com as escolhas do agente e seus momentos de aleatoriedade. Vamos ver como vai ser a convergencia depois de vários episódios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRy1ZNH01EIw",
        "outputId": "0da16c9c-9cc6-4ffa-e1bb-e25f3206157c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parâmetros\n",
        "states = 4\n",
        "goal_position = states-1\n",
        "actions = 2  # 0: esquerda, 1: direita\n",
        "q_table = np.zeros((states, actions))\n",
        "alpha = 0.1  # Taxa de aprendizado\n",
        "gamma = 0.9  # Fator de desconto\n",
        "epsilon = 0.2  # Taxa de exploração\n",
        "\n",
        "# Ambiente simples\n",
        "rewards = -1*np.ones(states-1)  # Recompensa ao alcançar o estado final\n",
        "rewards = np.append(rewards, 10)\n",
        "\n",
        "print(f\"Os valores de recompensa por estado são:\\n{rewards}\\n\")\n",
        "print(f\"A tabela Q foi inicializada como:\\n{q_table}\")\n",
        "\n",
        "# Treinamento\n",
        "for episode in range(10000):\n",
        "    state = 0  # Estado inicial\n",
        "    print(f\"A tabela Q antes de iniciar o episódio {episode+1}:\\n{q_table}\")\n",
        "    while state != goal_position:\n",
        "        # Escolha da ação (exploração/exploração)\n",
        "        if np.random.random() < epsilon:\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        # Próximo estado e recompensa\n",
        "        next_state = state + 1 if action == 1 else max(0, state - 1)\n",
        "        reward = rewards[next_state]\n",
        "\n",
        "        # Atualização Q-Learning\n",
        "        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "        state = next_state\n",
        "\n",
        "# Resultados\n",
        "print(f\"Tabela Q final, depois de {episode+1} episódios:\\n{q_table}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erZZZJ26dFcs"
      },
      "source": [
        "Tabela Q final (1 episódio):\n",
        "- [[-0.3529 -0.271 ]\n",
        "-  [-0.2152 -0.19  ]\n",
        "-  [-0.109   1.    ]\n",
        "-  [ 0.      0.    ]]\n",
        "\n",
        "Tabela Q final, depois de 10 episódios:\n",
        "- [[-0.58519851 -0.35079619]\n",
        "-  [-0.4477676   1.68892023]\n",
        "-  [-0.109       6.5132156 ]\n",
        "-  [ 0.          0.        ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrOLwFgm2DyC"
      },
      "source": [
        "### Q-learning em um ambiente mais complexo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_dhLH6v2J_P"
      },
      "source": [
        "Vamos ver como fica a configuração de um ambiente mais complexo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1WgftR22RZt"
      },
      "outputs": [],
      "source": [
        "def step(current_position, action):\n",
        "    \"\"\"\n",
        "    Modela um passo no ambiente FrozenLake 4x4.\n",
        "\n",
        "    Args:\n",
        "        current_position: Posição atual do agente (0-15).\n",
        "        action: Ação a ser realizada (0: esquerda, 1: baixo, 2: direita, 3: cima).\n",
        "\n",
        "    Returns:\n",
        "        Uma tupla contendo:\n",
        "            - next_position: A próxima posição do agente.\n",
        "            - reward: A recompensa obtida.\n",
        "            - done: Um booleano indicando se o jogo acabou.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define o mapa do FrozenLake 4x4 (S: Start, F: Frozen, H: Hole, G: Goal)\n",
        "    lake = np.array([\n",
        "        ['S', 'F', 'F', 'F'],\n",
        "        ['F', 'H', 'F', 'H'],\n",
        "        ['F', 'F', 'F', 'H'],\n",
        "        ['H', 'F', 'F', 'G']\n",
        "    ])\n",
        "\n",
        "    # Converte a posição para coordenadas (linha, coluna)\n",
        "    row, col = divmod(current_position, 4)\n",
        "\n",
        "    # Aplica a ação e calcula a próxima posição\n",
        "    if action == 0:  # Esquerda\n",
        "        col = max(0, col - 1)\n",
        "    elif action == 1:  # Baixo\n",
        "        row = min(3, row + 1)\n",
        "    elif action == 2:  # Direita\n",
        "        col = min(3, col + 1)\n",
        "    elif action == 3:  # Cima\n",
        "        row = max(0, row - 1)\n",
        "\n",
        "    next_position = row * 4 + col\n",
        "\n",
        "    # Calcula a recompensa e verifica se o jogo terminou\n",
        "    if lake[row, col] == 'G':\n",
        "        reward = 10\n",
        "        done = True\n",
        "    elif lake[row, col] == 'H':\n",
        "        reward = -1\n",
        "        done = True\n",
        "    else:\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "    return next_position, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJlBjtBN2JVR",
        "outputId": "58950d26-29a3-4011-824a-00b63b8f1ed0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parâmetros\n",
        "states = 16 # labirinto 4x4. Posição do agente (0-15)\n",
        "actions = 4  # action: Ação a ser realizada (0: esquerda, 1: baixo, 2: direita, 3: cima)\n",
        "q_table = np.zeros((states, actions))\n",
        "alpha = 0.1  # Taxa de aprendizado\n",
        "gamma = 0.9  # Fator de desconto\n",
        "epsilon = 0.2  # Taxa de exploração\n",
        "\n",
        "print(f\"A tabela Q foi inicializada como:\\n{q_table}\")\n",
        "\n",
        "# Treinamento\n",
        "for episode in range(10):\n",
        "    state = 0  # Estado inicial\n",
        "    #print(f\"A tabela Q antes de iniciar o episódio {episode+1}:\\n{q_table}\")\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not(done):\n",
        "        # Escolha da ação (explotação/exploração)\n",
        "        if np.random.random() < epsilon:\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        # Próximo estado, recompensa e status do episodio\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # Atualização Q-Learning\n",
        "        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "        state = next_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dZ0LnqDv0fR",
        "outputId": "842a8422-d35e-4053-f914-7a5c2b0847d4"
      },
      "outputs": [],
      "source": [
        "q_table\n",
        "# action: Ação a ser realizada (0: esquerda, 1: baixo, 2: direita, 3: cima)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUWEewMi66Kh",
        "outputId": "fb666425-a8b8-41ff-da2e-bb5ef86775be"
      },
      "outputs": [],
      "source": [
        "lake = np.array([\n",
        "        ['S(0)',  'F(1)',  'F(2)',  'F(3)'],\n",
        "        ['F(4)',  'H(5)',  'F(6)',  'H(7)'],\n",
        "        ['F(8)',  'F(9)',  'F(10)', 'H(11)'],\n",
        "        ['H(12)', 'F(13)', 'F(14)', 'G(15)']\n",
        "    ])\n",
        "# Resultados\n",
        "# action: Ação a ser realizada (0: esquerda, 1: baixo, 2: direita, 3: cima)\n",
        "print(f\"Tabela Q final, depois de {episode+1} episódios:\\n{q_table}\")\n",
        "print(f\"Gabarito do labirinto:\\n{lake}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYluU7w98RZU"
      },
      "source": [
        "Visualizar a tabela Q em forma de dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "GC5kk32Y7iZH",
        "outputId": "b426d68b-6b17-46b8-f9ea-ac1034a192a3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Resultados\n",
        "# action: Ação a ser realizada (0: esquerda, 1: baixo, 2: direita, 3: cima)\n",
        "\n",
        "# Converter a tabela Q para um dataframe Pandas\n",
        "q_table_df = pd.DataFrame(q_table, columns=['Esquerda', 'Baixo', 'Direita', 'Cima'])\n",
        "\n",
        "# Mostrar o dataframe como um mapa de calor\n",
        "print(\"\\nQ-table as DataFrame:\")\n",
        "styled_q_table = q_table_df.style.background_gradient(cmap='Blues',axis=1)\n",
        "display(styled_q_table)\n",
        "print(f\"Gabarito do labirinto:\\n{lake}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NRSgYOi-up4"
      },
      "source": [
        "Esta é a melhor tabela Q que o algoritmo conseguiu alcançar."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
